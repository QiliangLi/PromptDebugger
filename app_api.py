import streamlit as st
import os
import json
import time
import datetime
import difflib
import re
from jinja2 import Template
from openai import OpenAI

# ==========================================
# 1. é…ç½®ä¸åˆå§‹åŒ–
# ==========================================

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="PromptCraft - æ™ºèƒ½æç¤ºè¯è°ƒè¯•å·¥ä½œå°",
    page_icon="ğŸ› ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ç›®å½•åˆå§‹åŒ–
SESSION_DIR = "prompt_sessions"
os.makedirs(SESSION_DIR, exist_ok=True)

# Session State åˆå§‹åŒ–
if 'history' not in st.session_state:
    st.session_state.history = []
if 'logs' not in st.session_state:
    st.session_state.logs = []
if 'current_prompt' not in st.session_state:
    st.session_state.current_prompt = ""
if 'iteration_count' not in st.session_state:
    st.session_state.iteration_count = 0
if 'is_running' not in st.session_state:
    st.session_state.is_running = False

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ç±» (åç«¯æœåŠ¡)
# ==========================================

class DataManager:
    """æ•°æ®æŒä¹…åŒ–å±‚ï¼šä¸ä¾èµ–æ•°æ®åº“ï¼Œä½¿ç”¨JSONæ–‡ä»¶"""
    @staticmethod
    def save_session(session_data, prefix="session"):
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{prefix}_{timestamp}.json"
        path = os.path.join(SESSION_DIR, filename)
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            return path
        except Exception as e:
            st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
            return None

class LLMEngine:
    """LLM è°ƒç”¨å¼•æ“ï¼šé›†æˆ OpenAI SDK"""
    def __init__(self, api_key, base_url=None):
        self.client = None
        self.mock = False
        if not api_key:
            self.mock = True
        else:
            try:
                self.client = OpenAI(
                    api_key=api_key,
                    # base_url=base_url if base_url and base_url.strip() else "https://api.openai.com/v1"
                    base_url="https://api-inference.modelscope.cn/v1"
                )
            except Exception as e:
                st.error(f"API åˆå§‹åŒ–å¤±è´¥: {e}")
                self.mock = True

    def _clean_json(self, text):
        """æ¸…æ´—å¹¶æå– JSON"""
        try:
            return json.loads(text)
        except:
            match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
            if match:
                try: return json.loads(match.group(1))
                except: pass
            match = re.search(r"(\{.*\})", text, re.DOTALL)
            if match:
                try: return json.loads(match.group(1))
                except: pass
            return None

    def execute(self, prompt, config):
        """æ‰§è¡Œ Prompt"""
        if self.mock:
            time.sleep(1)
            return f"ã€æ¨¡æ‹Ÿç»“æœã€‘æ ¹æ® Prompt: {prompt[:30]}... ç”Ÿæˆçš„å›ç­”ã€‚\nè®¾ç½®æ¸©åº¦: {config.get('temperature')}"
        
        try:
            response = self.client.chat.completions.create(
                model=config.get("model", "gpt-3.5-turbo"),
                messages=[{"role": "user", "content": prompt}],
                temperature=config.get("temperature", 0.7),
                max_tokens=config.get("max_tokens", 1000),
                stream=False,
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Execute Error: {str(e)}"

    def evaluate(self, prompt, output, expected, config):
        """è¯„ä¼°ç»“æœè´¨é‡"""
        if self.mock:
            time.sleep(0.5)
            import random
            score = random.randint(60, 95)
            return {"score": score, "reason": "æ¨¡æ‹Ÿè¯„åˆ†æ¨¡å¼ï¼šç»“æœç»“æ„å°šå¯ï¼Œä½†ç»†èŠ‚éœ€ä¼˜åŒ–ã€‚"}

        eval_prompt = f"""
        ä½ æ˜¯ä¸€åä¸¥æ ¼çš„è¯„åˆ¤å‘˜ã€‚è¯·æ ¹æ®é¢„æœŸæ ‡å‡†è¯„ä¼°å®é™…è¾“å‡ºã€‚
        
        ã€Promptã€‘: {prompt}
        ã€å®é™…è¾“å‡ºã€‘: {output}
        ã€é¢„æœŸè¾“å‡º/æ ‡å‡†ã€‘: {expected}
        
        è¯·è¾“å‡º JSON æ ¼å¼ï¼š
        {{
            "score": <0-100çš„æ•´æ•°>,
            "reason": "<ç®€çŸ­è¯„è¯­>"
        }}
        """
        try:
            response = self.client.chat.completions.create(
                model=config.get("model", "gpt-4"),
                messages=[{"role": "user", "content": eval_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1,
                stream=False,
            )
            res = self._clean_json(response.choices[0].message.content)
            return res if res else {"score": 0, "reason": "è¯„åˆ†æ ¼å¼è§£æå¤±è´¥"}
        except Exception as e:
            return {"score": 0, "reason": f"Evaluate Error: {str(e)}"}

    def optimize(self, current_prompt, eval_result, config):
        """åŸºäºè¯„ä¼°ä¼˜åŒ– Prompt"""
        if self.mock:
            time.sleep(1)
            return current_prompt + "\n\n(å·²åŸºäºè¯„ä¼°ç»“æœè‡ªåŠ¨ä¼˜åŒ–ï¼šå¢åŠ äº†å…·ä½“çš„æ ¼å¼é™åˆ¶)"

        opt_prompt = f"""
        ä½ æ˜¯ä¸€å Prompt å·¥ç¨‹å¸ˆã€‚è¯·æ ¹æ®è¯„åˆ†ä¼˜åŒ–æç¤ºè¯ã€‚
        
        ã€åŸæç¤ºè¯ã€‘: {current_prompt}
        ã€è¯„åˆ†ã€‘: {eval_result.get('score')}
        ã€é—®é¢˜ã€‘: {eval_result.get('reason')}
        
        è¦æ±‚ï¼š
        1. ä¿æŒåŸæœ‰çš„ {{{{variable}}}} å ä½ç¬¦ä¸å˜ã€‚
        2. ä½¿ç”¨æ€ç»´é“¾æˆ–å°‘æ ·æœ¬æŠ€å·§å¢å¼ºæ•ˆæœã€‚
        3. ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„æç¤ºè¯å†…å®¹ï¼Œæ— éœ€è§£é‡Šã€‚
        """
        try:
            response = self.client.chat.completions.create(
                model=config.get("model", "gpt-4"),
                messages=[{"role": "user", "content": opt_prompt}],
                temperature=0.7,
                stream=False,
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Optimize Error: {str(e)}"

# ==========================================
# 3. è¾…åŠ©å‡½æ•°
# ==========================================

def log(msg):
    t = datetime.datetime.now().strftime("%H:%M:%S")
    st.session_state.logs.append(f"[{t}] {msg}")

def render_prompt(template, variables_str):
    if not variables_str.strip():
        return template
    try:
        vars_dict = json.loads(variables_str)
        return Template(template).render(**vars_dict)
    except Exception as e:
        log(f"æ¸²æŸ“é”™è¯¯: {e}")
        return template

def get_diff_html(text1, text2):
    d = difflib.Differ()
    diff = list(d.compare(text1.splitlines(), text2.splitlines()))
    html = []
    for line in diff:
        if line.startswith('+ '):
            html.append(f'<div style="background:#e6ffec;color:#155724;padding:2px;">{line}</div>')
        elif line.startswith('- '):
            html.append(f'<div style="background:#ffe6e6;color:#721c24;padding:2px;">{line}</div>')
        elif line.startswith('? '): continue
        else: html.append(f'<div style="color:#666;padding:2px;">{line}</div>')
    return "".join(html)

# ==========================================
# 4. ç•Œé¢æ„å»º
# ==========================================

# --- ä¾§è¾¹æ ï¼šé…ç½® ---
with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®é¢æ¿")
    
    st.subheader("1. API é…ç½®")
    api_key = st.text_input("OpenAI API Key", placeholder="ms-06251eb2-c784-4b06-9009-9f7f2bd61602", type="password", help="ç•™ç©ºåˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    base_url = st.text_input("Base URL (å¯é€‰)", placeholder="https://api-inference.modelscope.cn/v1")
    
    st.subheader("2. æ¨¡å‹è·¯ç”±")
    c1, c2 = st.columns(2)
    exec_model = c1.selectbox("æ‰§è¡Œæ¨¡å‹", ["gpt-3.5-turbo", "gpt-4o", "gpt-4", 'Qwen/Qwen3-8B', 'deepseek-ai/DeepSeek-V3.1'], index=0)
    exec_temp = c2.slider("æ‰§è¡Œæ¸©åº¦", 0.0, 1.0, 0.7)
    
    eval_model = st.selectbox("è¯„ä»·æ¨¡å‹", ["gpt-4", "gpt-4o", 'Qwen/Qwen3-8B', 'deepseek-ai/DeepSeek-V3.1'], index=0, help="å»ºè®®ä½¿ç”¨å¼ºæ¨¡å‹è¿›è¡Œè¯„åˆ†")
    opt_model = st.selectbox("ä¼˜åŒ–æ¨¡å‹", ["gpt-4", "gpt-4o", 'Qwen/Qwen3-8B', 'deepseek-ai/DeepSeek-V3.1'], index=0, help="å»ºè®®ä½¿ç”¨å¼ºæ¨¡å‹è¿›è¡Œé‡å†™")
    
    st.divider()
    st.info(f"æ¨¡å¼: {'ğŸ”µ æ¨¡æ‹Ÿæ¨¡å¼' if not api_key else 'ğŸŸ¢ API æ¨¡å¼'}")
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²"):
        st.session_state.history = []
        st.session_state.logs = []
        st.session_state.iteration_count = 0
        st.session_state.current_prompt = ""
        st.rerun()

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ› ï¸ PromptCraft è°ƒè¯•å·¥å…·")

# è¾“å…¥åŒºåŸŸ
col_input, col_config = st.columns([3, 2])

with col_input:
    st.subheader("æç¤ºè¯æ¨¡æ¿ (å¯å˜éƒ¨åˆ†ç”¨ {{var}})")
    # å¦‚æœè¿˜æ²¡æœ‰å½“å‰Promptï¼Œä½¿ç”¨é»˜è®¤å€¼
    default_prompt = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ã€‚\nè¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š\n{{text}}"
    if not st.session_state.current_prompt:
        st.session_state.current_prompt = default_prompt
        
    prompt_input = st.text_area("Prompt Input", value=st.session_state.current_prompt, height=250, key="ui_prompt_input")
    # åŒæ­¥å› session_state (å…è®¸äººå·¥æ‰‹åŠ¨ä¿®æ”¹)
    st.session_state.current_prompt = prompt_input

with col_config:
    st.subheader("æµ‹è¯•æ•°æ® & é¢„æœŸ")
    vars_input = st.text_area("å˜é‡ (JSON)", value='{\n  "text": "The quick brown fox jumps over the lazy dog."\n}', height=100)
    expect_input = st.text_area("é¢„æœŸå›ç­” (ç”¨äºè¯„åˆ†)", value="è¿™åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†è¿™åªæ‡’æƒ°çš„ç‹—ã€‚", height=100)
    
    st.markdown("---")
    c_iter, c_mode = st.columns(2)
    max_iters = c_iter.number_input("è¿­ä»£æ¬¡æ•°", 1, 10, 3)
    mode = c_mode.radio("è¿­ä»£æ¨¡å¼", ["è‡ªåŠ¨è¿ç»­", "äº¤äº’å¼(å•æ­¥)"], horizontal=True)
    
    auto_rollback = st.checkbox("ğŸ“‰ å¯ç”¨è‡ªåŠ¨å›æ»š (åˆ†æ•°ä¸‹é™æ—¶æ¢å¤)", value=True)

# æ“ä½œæ 
st.divider()
b1, b2, b3 = st.columns([1, 1, 4])
start_btn = b1.button("â–¶ï¸ å¼€å§‹è°ƒè¯•", type="primary", use_container_width=True)
stop_btn = b2.button("â¹ï¸ åœæ­¢", use_container_width=True)

# ==========================================
# 5. æ‰§è¡Œé€»è¾‘
# ==========================================

engine = LLMEngine(api_key, base_url)

if start_btn:
    st.session_state.is_running = True
    st.session_state.iteration_count = 0
    # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—ï¼Œä¿ç•™å†å²è®°å½•å¯é€‰ï¼Œè¿™é‡Œé€‰æ‹©æ¸…ç©ºæœ¬æ¬¡ä¼šè¯çš„ä¸´æ—¶æ—¥å¿—
    st.session_state.logs = [] 

if st.session_state.is_running:
    status_container = st.status("æ­£åœ¨æ‰§è¡Œè°ƒè¯•æµç¨‹...", expanded=True)
    progress_bar = st.progress(0)
    
    # ç¡®å®šå¾ªç¯æ¬¡æ•°ï¼šå¦‚æœæ˜¯äº¤äº’å¼ï¼Œå®é™…ä¸Šåªè·‘ 1 è½®ï¼Œä½†ä¸ºäº†ä»£ç å¤ç”¨ï¼Œæˆ‘ä»¬åœ¨å¾ªç¯å†…æ§åˆ¶ break
    target_loops = max_iters if mode == "è‡ªåŠ¨è¿ç»­" else 1
    
    current_p = st.session_state.current_prompt
    
    for i in range(target_loops):
        idx = st.session_state.iteration_count + 1
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§æ¬¡æ•°ï¼Œåœæ­¢
        if idx > max_iters:
            st.success("è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚")
            st.session_state.is_running = False
            break

        status_container.write(f"ğŸ”„ æ­£åœ¨æ‰§è¡Œç¬¬ {idx} è½®è¿­ä»£...")
        log(f"=== å¼€å§‹ç¬¬ {idx} è½® ===")
        
        # 1. æ¸²æŸ“
        rendered_prompt = render_prompt(current_p, vars_input)
        
        # 2. æ‰§è¡Œ
        log("è°ƒç”¨ LLM æ‰§è¡Œ...")
        output = engine.execute(rendered_prompt, {"model": exec_model, "temperature": exec_temp})
        
        # 3. è¯„ä»·
        log("è°ƒç”¨ LLM è¯„åˆ†...")
        eval_res = engine.evaluate(rendered_prompt, output, expect_input, {"model": eval_model})
        score = eval_res.get('score', 0)
        log(f"æœ¬è½®å¾—åˆ†: {score}")
        
        # è®°å½•æ•°æ®
        record = {
            "version": idx,
            "prompt_template": current_p,
            "rendered_prompt": rendered_prompt,
            "output": output,
            "evaluation": eval_res,
            "timestamp": datetime.datetime.now().isoformat(),
            "status": "normal"
        }
        
        # 4. å›æ»šåˆ¤æ–­
        rollback_triggered = False
        if auto_rollback and len(st.session_state.history) > 0:
            last_score = st.session_state.history[-1]['evaluation']['score']
            if score < last_score:
                log(f"âš ï¸ åˆ†æ•°ä¸‹é™ ({score} < {last_score})ï¼Œè§¦å‘å›æ»šã€‚")
                record['status'] = "rolled_back"
                rollback_triggered = True
                # å›æ»šæç¤ºè¯ï¼šé‡ç½®ä¸ºä¸Šä¸€ä¸ªæœ‰æ•ˆç‰ˆæœ¬
                current_p = st.session_state.history[-1]['prompt_template']
            else:
                record['status'] = "accepted"
        else:
            record['status'] = "accepted"
            
        st.session_state.history.append(record)
        st.session_state.iteration_count += 1
        progress_bar.progress(idx / max_iters)
        
        # 5. ä¼˜åŒ– (å¦‚æœæ˜¯æœ€åä¸€è½®åˆ™ä¸ä¼˜åŒ–ï¼Œæˆ–è€…è§¦å‘å›æ»šåä¸åŸºäºå½“å‰åç»“æœä¼˜åŒ–)
        if idx < max_iters:
            if mode == "äº¤äº’å¼(å•æ­¥)":
                log("äº¤äº’æ¨¡å¼ï¼šæš‚åœç­‰å¾…ç”¨æˆ·ç¡®è®¤...")
                st.session_state.is_running = False # åœæ­¢è¿è¡Œï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å†æ¬¡ç‚¹å‡»
                st.info(f"ç¬¬ {idx} è½®å®Œæˆã€‚è¯·æŸ¥çœ‹ç»“æœï¼Œå¦‚æœéœ€è¦ä¼˜åŒ–ä¸‹ä¸€ç‰ˆï¼Œè¯·å†æ¬¡ç‚¹å‡»å¼€å§‹ã€‚")
                break
            
            if not rollback_triggered:
                log("ç”Ÿæˆä¼˜åŒ–å»ºè®®ä¸­...")
                optimized_prompt = engine.optimize(current_p, eval_res, {"model": opt_model})
                current_p = optimized_prompt
                st.session_state.current_prompt = optimized_prompt # æ›´æ–°å…¨å±€çŠ¶æ€
        else:
            st.session_state.is_running = False
            st.success("æ‰€æœ‰è¿­ä»£å·²å®Œæˆï¼")

    status_container.update(label="æµç¨‹ç»“æŸ", state="complete", expanded=False)

# ==========================================
# 6. ç»“æœå±•ç¤ºé¢æ¿
# ==========================================

st.divider()

if not st.session_state.history:
    st.info("æš‚æ— æ•°æ®ï¼Œè¯·ç‚¹å‡»å¼€å§‹è°ƒè¯•ã€‚")
else:
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ç»“æœè¯¦æƒ…", "ğŸ“ˆ è¶‹åŠ¿åˆ†æ", "ğŸ“ å·®å¼‚å¯¹æ¯”", "ğŸ“Ÿ è¿è¡Œæ—¥å¿—"])
    
    with tab1:
        # ç‰ˆæœ¬é€‰æ‹©å™¨
        versions = [f"v{h['version']} (Score: {h['evaluation']['score']}) {'â†©ï¸' if h['status']=='rolled_back' else ''}" for h in st.session_state.history]
        sel_idx = st.selectbox("é€‰æ‹©ç‰ˆæœ¬æŸ¥çœ‹", range(len(versions)), format_func=lambda x: versions[x])
        data = st.session_state.history[sel_idx]
        
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("#### ğŸ“ æç¤ºè¯æ¨¡æ¿")
            st.code(data['prompt_template'], language='markdown')
            
            st.markdown("#### ğŸ¤– å®é™…è¾“å‡º")
            st.info(data['output'])
            
        with c2:
            st.markdown("#### ğŸ† è´¨é‡è¯„ä¼°")
            score = data['evaluation']['score']
            st.metric("å¾—åˆ†", score, delta=None)
            st.warning(f"**è¯„ä»·ç†ç”±**: {data['evaluation']['reason']}")
            
            st.markdown("---")
            st.markdown("#### ğŸ”§ äººå·¥å¹²é¢„")
            # å…è®¸ç”¨æˆ·åŸºäºæ­¤ç‰ˆæœ¬ä¿®æ”¹
            manual_edit = st.text_area("åœ¨æ­¤ç‰ˆæœ¬åŸºç¡€ä¸Šä¿®æ”¹:", value=data['prompt_template'], key=f"manual_{sel_idx}")
            if st.button("é‡‡çº³æ­¤ä¿®æ”¹å¹¶è¦†ç›–è¾“å…¥æ¡†", key=f"btn_{sel_idx}"):
                st.session_state.current_prompt = manual_edit
                st.rerun()

    with tab2:
        if len(st.session_state.history) > 0:
            scores = [h['evaluation']['score'] for h in st.session_state.history]
            st.line_chart(scores)
            
            # æ•°æ®è¡¨å¯¼å‡º
            import pandas as pd
            df = pd.DataFrame([{
                "Version": h['version'],
                "Score": h['evaluation']['score'],
                "Output": h['output'][:50]+"...",
                "Status": h['status']
            } for h in st.session_state.history])
            st.dataframe(df, use_container_width=True)

    with tab3:
        if len(st.session_state.history) >= 2:
            v_a = st.selectbox("ç‰ˆæœ¬ A", range(len(versions)), index=len(versions)-2, key="diff_a")
            v_b = st.selectbox("ç‰ˆæœ¬ B", range(len(versions)), index=len(versions)-1, key="diff_b")
            
            txt_a = st.session_state.history[v_a]['prompt_template']
            txt_b = st.session_state.history[v_b]['prompt_template']
            
            st.markdown("### å·®å¼‚è§†å›¾ (A vs B)")
            st.markdown(get_diff_html(txt_a, txt_b), unsafe_allow_html=True)
        else:
            st.warning("éœ€è¦è‡³å°‘ä¸¤ä¸ªç‰ˆæœ¬æ‰èƒ½è¿›è¡Œå¯¹æ¯”")

    with tab4:
        st.text_area("ç³»ç»Ÿæ—¥å¿—", value="\n".join(st.session_state.logs), height=300)

    # å¯¼å‡ºæŒ‰é’®
    report_data = json.dumps(st.session_state.history, indent=2, ensure_ascii=False)
    st.download_button("ğŸ’¾ å¯¼å‡ºå®Œæ•´æŠ¥å‘Š (JSON)", report_data, file_name="debug_report.json", mime="application/json")