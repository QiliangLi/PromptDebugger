import streamlit as st
import os
import json
import time
import datetime
import difflib
import re
from jinja2 import Template
from openai import OpenAI

# ==========================================
# 1. é…ç½®ä¸åˆå§‹åŒ–
# ==========================================

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="PromptCraft - æ™ºèƒ½æç¤ºè¯è°ƒè¯•å·¥ä½œå°",
    page_icon="ğŸ› ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ç›®å½•åˆå§‹åŒ–
SESSION_DIR = "prompt_sessions"
os.makedirs(SESSION_DIR, exist_ok=True)

# Session State åˆå§‹åŒ–
if 'history' not in st.session_state:
    st.session_state.history = []
if 'logs' not in st.session_state:
    st.session_state.logs = []
if 'current_prompt' not in st.session_state:
    st.session_state.current_prompt = ""
if 'iteration_count' not in st.session_state:
    st.session_state.iteration_count = 0
if 'is_running' not in st.session_state:
    st.session_state.is_running = False

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ç±» (åç«¯æœåŠ¡)
# ==========================================

class DataManager:
    """æ•°æ®æŒä¹…åŒ–å±‚ï¼šä¸ä¾èµ–æ•°æ®åº“ï¼Œä½¿ç”¨JSONæ–‡ä»¶"""
    @staticmethod
    def save_session(session_data, prefix="session"):
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{prefix}_{timestamp}.json"
        path = os.path.join(SESSION_DIR, filename)
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            return path
        except Exception as e:
            st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
            return None

class LLMEngine:
    """LLM è°ƒç”¨å¼•æ“ï¼šé›†æˆ OpenAI SDK"""
    def __init__(self, api_key, base_url=None):
        self.client = None
        self.mock = False
        if not api_key:
            self.mock = True
        else:
            try:
                self.client = OpenAI(
                    api_key=api_key,
                    # base_url=base_url if base_url and base_url.strip() else "https://api.openai.com/v1"
                    base_url="https://api-inference.modelscope.cn/v1"
                )
            except Exception as e:
                st.error(f"API åˆå§‹åŒ–å¤±è´¥: {e}")
                self.mock = True

    def _clean_json(self, text):
        """æ¸…æ´—å¹¶æå– JSON"""
        try:
            return json.loads(text)
        except:
            match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
            if match:
                try: return json.loads(match.group(1))
                except: pass
            match = re.search(r"(\{.*\})", text, re.DOTALL)
            if match:
                try: return json.loads(match.group(1))
                except: pass
            return None

    def execute(self, prompt, config):
        """æ‰§è¡Œ Prompt"""
        if self.mock:
            time.sleep(1)
            return f"ã€æ¨¡æ‹Ÿç»“æœã€‘æ ¹æ® Prompt: {prompt[:30]}... ç”Ÿæˆçš„å›ç­”ã€‚\nè®¾ç½®æ¸©åº¦: {config.get('temperature')}"
        
        try:
            response = self.client.chat.completions.create(
                model=config.get("model", "gpt-3.5-turbo"),
                messages=[{"role": "user", "content": prompt}],
                temperature=config.get("temperature", 0.7),
                max_tokens=config.get("max_tokens", 1000),
                stream=False,
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Execute Error: {str(e)}"

    def evaluate(self, prompt, output, expected, config):
        """è¯„ä¼°ç»“æœè´¨é‡"""
        if self.mock:
            time.sleep(0.5)
            import random
            score = random.randint(60, 95)
            return {"score": score, "reason": "æ¨¡æ‹Ÿè¯„åˆ†æ¨¡å¼ï¼šç»“æœç»“æ„å°šå¯ï¼Œä½†ç»†èŠ‚éœ€ä¼˜åŒ–ã€‚"}

        eval_prompt = f"""
        ä½ æ˜¯ä¸€åä¸¥æ ¼çš„è¯„åˆ¤å‘˜ã€‚è¯·æ ¹æ®é¢„æœŸæ ‡å‡†è¯„ä¼°å®é™…è¾“å‡ºã€‚
        
        ã€Promptã€‘: {prompt}
        ã€å®é™…è¾“å‡ºã€‘: {output}
        ã€é¢„æœŸè¾“å‡º/æ ‡å‡†ã€‘: {expected}
        
        è¯·è¾“å‡º JSON æ ¼å¼ï¼š
        {{
            "score": <0-100çš„æ•´æ•°>,
            "reason": "<ç®€çŸ­è¯„è¯­>"
        }}
        """
        try:
            response = self.client.chat.completions.create(
                model=config.get("model", "gpt-4"),
                messages=[{"role": "user", "content": eval_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1,
                stream=False,
            )
            res = self._clean_json(response.choices[0].message.content)
            return res if res else {"score": 0, "reason": "è¯„åˆ†æ ¼å¼è§£æå¤±è´¥"}
        except Exception as e:
            return {"score": 0, "reason": f"Evaluate Error: {str(e)}"}

    def optimize(self, current_prompt, eval_result, config):
        """åŸºäºè¯„ä¼°ä¼˜åŒ– Prompt"""
        if self.mock:
            time.sleep(1)
            return current_prompt + "\n\n(å·²åŸºäºè¯„ä¼°ç»“æœè‡ªåŠ¨ä¼˜åŒ–ï¼šå¢åŠ äº†å…·ä½“çš„æ ¼å¼é™åˆ¶)"

        opt_prompt = f"""
        ä½ æ˜¯ä¸€å Prompt å·¥ç¨‹å¸ˆã€‚è¯·æ ¹æ®è¯„åˆ†ä¼˜åŒ–æç¤ºè¯ã€‚
        
        ã€åŸæç¤ºè¯ã€‘: {current_prompt}
        ã€è¯„åˆ†ã€‘: {eval_result.get('score')}
        ã€é—®é¢˜ã€‘: {eval_result.get('reason')}
        
        è¦æ±‚ï¼š
        1. ä¿æŒåŸæœ‰çš„ {{{{variable}}}} å ä½ç¬¦ä¸å˜ã€‚
        2. ä½¿ç”¨æ€ç»´é“¾æˆ–å°‘æ ·æœ¬æŠ€å·§å¢å¼ºæ•ˆæœã€‚
        3. ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„æç¤ºè¯å†…å®¹ï¼Œæ— éœ€è§£é‡Šã€‚
        """
        try:
            response = self.client.chat.completions.create(
                model=config.get("model", "gpt-4"),
                messages=[{"role": "user", "content": opt_prompt}],
                temperature=0.7,
                stream=False,
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Optimize Error: {str(e)}"

# ==========================================
# 3. è¾…åŠ©å‡½æ•°
# ==========================================

def log(msg):
    t = datetime.datetime.now().strftime("%H:%M:%S")
    st.session_state.logs.append(f"[{t}] {msg}")

def render_prompt(template, variables_str):
    if not variables_str.strip():
        return template
    try:
        vars_dict = json.loads(variables_str)
        return Template(template).render(**vars_dict)
    except Exception as e:
        log(f"æ¸²æŸ“é”™è¯¯: {e}")
        return template

def get_diff_html(text1, text2):
    d = difflib.Differ()
    diff = list(d.compare(text1.splitlines(), text2.splitlines()))
    html = []
    for line in diff:
        if line.startswith('+ '):
            html.append(f'<div style="background:#e6ffec;color:#155724;padding:2px;">{line}</div>')
        elif line.startswith('- '):
            html.append(f'<div style="background:#ffe6e6;color:#721c24;padding:2px;">{line}</div>')
        elif line.startswith('? '): continue
        else: html.append(f'<div style="color:#666;padding:2px;">{line}</div>')
    return "".join(html)

# ==========================================
# 4. ç•Œé¢æ„å»º
# ==========================================

# --- ä¾§è¾¹æ ï¼šé…ç½® ---
with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®é¢æ¿")

    st.subheader("1. API é…ç½®")
    # è®¾ç½®é»˜è®¤APIå¯†é’¥å’ŒBase URLï¼Œä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    default_api_key = os.getenv("OPENAI_API_KEY", "ms-06251eb2-c784-4b06-9009-9f7f2bd61602")  # é»˜è®¤ä¸ºç©ºï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
    default_base_url = os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1")

    api_key = st.text_input("OpenAI API Key", placeholder="ms-06251eb2-c784-4b06-9009-9f7f2bd61602", help="ç•™ç©ºåˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼", value=default_api_key)
    base_url = st.text_input("Base URL (å¯é€‰)", placeholder="https://api-inference.modelscope.cn/v1", value=default_base_url)

    st.subheader("2. æ¨¡å‹è·¯ç”±")
    c1, c2 = st.columns(2)
    exec_model = c1.selectbox("æ‰§è¡Œæ¨¡å‹", ["gpt-3.5-turbo", "gpt-4o", "gpt-4", 'Qwen/Qwen3-8B', 'deepseek-ai/DeepSeek-V3.1'], index=0)
    exec_temp = c2.slider("æ‰§è¡Œæ¸©åº¦", 0.0, 1.0, 0.7)

    eval_model = st.selectbox("è¯„ä»·æ¨¡å‹", ["gpt-4", "gpt-4o", 'Qwen/Qwen3-8B', 'deepseek-ai/DeepSeek-V3.1'], index=0, help="å»ºè®®ä½¿ç”¨å¼ºæ¨¡å‹è¿›è¡Œè¯„åˆ†")
    opt_model = st.selectbox("ä¼˜åŒ–æ¨¡å‹", ["gpt-4", "gpt-4o", 'Qwen/Qwen3-8B', 'deepseek-ai/DeepSeek-V3.1'], index=0, help="å»ºè®®ä½¿ç”¨å¼ºæ¨¡å‹è¿›è¡Œé‡å†™")

    st.divider()
    st.info(f"æ¨¡å¼: {'ğŸ”µ æ¨¡æ‹Ÿæ¨¡å¼' if not api_key else 'ğŸŸ¢ API æ¨¡å¼'}")

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²"):
        st.session_state.history = []
        st.session_state.logs = []
        st.session_state.iteration_count = 0
        st.session_state.current_prompt = ""
        st.rerun()

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ› ï¸ PromptCraft è°ƒè¯•å·¥å…·")

# è¾“å…¥åŒºåŸŸ
col_input, col_config = st.columns([3, 2])

with col_input:
    st.subheader("æç¤ºè¯æ¨¡æ¿ (å¯å˜éƒ¨åˆ†ç”¨ {{var}})")
    # å¦‚æœè¿˜æ²¡æœ‰å½“å‰Promptï¼Œä½¿ç”¨é»˜è®¤å€¼
    default_prompt = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ã€‚\nè¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š\n{{text}}"
    if not st.session_state.current_prompt:
        st.session_state.current_prompt = default_prompt
        
    prompt_input = st.text_area("Prompt Input", value=st.session_state.current_prompt, height=250, key="ui_prompt_input")
    # åŒæ­¥å› session_state (å…è®¸äººå·¥æ‰‹åŠ¨ä¿®æ”¹)
    st.session_state.current_prompt = prompt_input

with col_config:
    st.subheader("æµ‹è¯•æ•°æ® & é¢„æœŸ")

    # ä½¿ç”¨JSONæ•°ç»„æ ¼å¼æ”¯æŒå¤šä¸ªæµ‹è¯•ç”¨ä¾‹
    st.markdown("**æµ‹è¯•ç”¨ä¾‹ (JSONæ•°ç»„æ ¼å¼)**")
    test_cases_input = st.text_area(
        "æµ‹è¯•ç”¨ä¾‹ (JSONæ•°ç»„)",
        value='[{\n  "name": "æµ‹è¯•ç”¨ä¾‹1",\n  "variables": {\n    "text": "The quick brown fox jumps over the lazy dog."\n  },\n  "expected": "è¿™åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†è¿™åªæ‡’æƒ°çš„ç‹—ã€‚"\n}]',
        height=200,
        help="è¾“å…¥JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«nameã€variableså’Œexpectedå­—æ®µ"
    )

    st.markdown("---")
    c_iter, c_mode = st.columns(2)
    max_iters = c_iter.number_input("è¿­ä»£æ¬¡æ•°", 1, 10, 3)
    mode = c_mode.radio("è¿­ä»£æ¨¡å¼", ["è‡ªåŠ¨è¿ç»­", "äº¤äº’å¼(å•æ­¥)"], horizontal=True)

    auto_rollback = st.checkbox("ğŸ“‰ å¯ç”¨è‡ªåŠ¨å›æ»š (åˆ†æ•°ä¸‹é™æ—¶æ¢å¤)", value=True)

# æ“ä½œæ 
st.divider()
b1, b2, b3 = st.columns([1, 1, 4])
start_btn = b1.button("â–¶ï¸ å¼€å§‹è°ƒè¯•", type="primary", use_container_width=True)
stop_btn = b2.button("â¹ï¸ åœæ­¢", use_container_width=True)

# ==========================================
# 5. æ‰§è¡Œé€»è¾‘
# ==========================================

engine = LLMEngine(api_key, base_url)

if start_btn:
    st.session_state.is_running = True
    st.session_state.iteration_count = 0
    # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—ï¼Œä¿ç•™å†å²è®°å½•å¯é€‰ï¼Œè¿™é‡Œé€‰æ‹©æ¸…ç©ºæœ¬æ¬¡ä¼šè¯çš„ä¸´æ—¶æ—¥å¿—
    st.session_state.logs = [] 

if st.session_state.is_running:
    status_container = st.status("æ­£åœ¨æ‰§è¡Œè°ƒè¯•æµç¨‹...", expanded=True)
    progress_bar = st.progress(0)
    
    # ç¡®å®šå¾ªç¯æ¬¡æ•°ï¼šå¦‚æœæ˜¯äº¤äº’å¼ï¼Œå®é™…ä¸Šåªè·‘ 1 è½®ï¼Œä½†ä¸ºäº†ä»£ç å¤ç”¨ï¼Œæˆ‘ä»¬åœ¨å¾ªç¯å†…æ§åˆ¶ break
    target_loops = max_iters if mode == "è‡ªåŠ¨è¿ç»­" else 1
    
    current_p = st.session_state.current_prompt
    
    for i in range(target_loops):
        idx = st.session_state.iteration_count + 1
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§æ¬¡æ•°ï¼Œåœæ­¢
        if idx > max_iters:
            st.success("è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚")
            st.session_state.is_running = False
            break

        status_container.write(f"ğŸ”„ æ­£åœ¨æ‰§è¡Œç¬¬ {idx} è½®è¿­ä»£...")
        log(f"=== å¼€å§‹ç¬¬ {idx} è½® ===")
        
        # 1. è§£ææµ‹è¯•ç”¨ä¾‹
        test_cases = []
        try:
            test_cases = json.loads(test_cases_input)
            # å¦‚æœä¸æ˜¯æ•°ç»„æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ•°ç»„
            if not isinstance(test_cases, list):
                test_cases = [test_cases]
        except Exception as e:
            log(f"æµ‹è¯•ç”¨ä¾‹æ ¼å¼é”™è¯¯: {str(e)}")
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹
            test_cases = [{
                "name": "é»˜è®¤æµ‹è¯•ç”¨ä¾‹",
                "variables": {"text": "The quick brown fox jumps over the lazy dog."},
                "expected": "è¿™åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†è¿™åªæ‡’æƒ°çš„ç‹—ã€‚"
            }]

        # 2. å¯¹æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„ä¼°
        total_score = 0
        eval_results = []

        for idx, test_case in enumerate(test_cases):
            case_name = test_case.get("name", f"æµ‹è¯•ç”¨ä¾‹ {idx+1}")
            log(f"æµ‹è¯•ç”¨ä¾‹ {idx+1}: {case_name}")

            # è·å–å˜é‡å’Œé¢„æœŸè¾“å‡º
            variables = test_case.get("variables", {})
            expected = test_case.get("expected", "")

            # å°†å˜é‡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆå¦‚æœéœ€è¦ï¼‰
            variables_str = variables if isinstance(variables, str) else json.dumps(variables, ensure_ascii=False)

            # æ¸²æŸ“
            rendered_prompt = render_prompt(current_p, variables_str)

            # æ‰§è¡Œ
            log("è°ƒç”¨ LLM æ‰§è¡Œ...")
            output = engine.execute(rendered_prompt, {"model": exec_model, "temperature": exec_temp})

            # è¯„ä»·
            log("è°ƒç”¨ LLM è¯„åˆ†...")
            eval_res = engine.evaluate(rendered_prompt, output, expected, {"model": eval_model})
            score = eval_res.get('score', 0)
            log(f"æµ‹è¯•ç”¨ä¾‹ {idx+1} å¾—åˆ†: {score}")

            eval_results.append({
                "test_case": case_name,
                "score": score,
                "result": eval_res,
                "output": output,
                "variables": variables_str,
                "expected": expected
            })
            total_score += score

        # è®¡ç®—å¹³å‡åˆ†
        avg_score = total_score / len(test_cases) if test_cases else 0
        log(f"æœ¬è½®å¹³å‡å¾—åˆ†: {avg_score}")

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ç»“æœä½œä¸ºä¸»è¦è¯„ä¼°ç»“æœï¼ˆç”¨äºä¼˜åŒ–ï¼‰
        eval_res = eval_results[0]["result"] if eval_results else {"score": 0, "reason": "æ— æµ‹è¯•ç”¨ä¾‹"}
        
        # è®°å½•æ•°æ®
        record = {
            "version": idx,
            "prompt_template": current_p,
            "test_results": eval_results,
            "test_cases_input": test_cases_input,  # ä¿å­˜åŸå§‹è¾“å…¥
            "average_score": avg_score,
            "evaluation": eval_res,  # ä¸»è¦è¯„ä¼°ç»“æœï¼ˆç¬¬ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼‰
            "timestamp": datetime.datetime.now().isoformat(),
            "status": "normal"
        }
        
        # 4. å›æ»šåˆ¤æ–­
        rollback_triggered = False
        if auto_rollback and len(st.session_state.history) > 0:
            last_score = st.session_state.history[-1]['evaluation']['score']
            if score < last_score:
                log(f"âš ï¸ åˆ†æ•°ä¸‹é™ ({score} < {last_score})ï¼Œè§¦å‘å›æ»šã€‚")
                record['status'] = "rolled_back"
                rollback_triggered = True
                # å›æ»šæç¤ºè¯ï¼šé‡ç½®ä¸ºä¸Šä¸€ä¸ªæœ‰æ•ˆç‰ˆæœ¬
                current_p = st.session_state.history[-1]['prompt_template']
            else:
                record['status'] = "accepted"
        else:
            record['status'] = "accepted"
            
        st.session_state.history.append(record)
        st.session_state.iteration_count += 1
        progress_bar.progress(idx / max_iters)
        
        # 5. ä¼˜åŒ– (å¦‚æœæ˜¯æœ€åä¸€è½®åˆ™ä¸ä¼˜åŒ–ï¼Œæˆ–è€…è§¦å‘å›æ»šåä¸åŸºäºå½“å‰åç»“æœä¼˜åŒ–)
        if idx < max_iters:
            if mode == "äº¤äº’å¼(å•æ­¥)":
                log("äº¤äº’æ¨¡å¼ï¼šæš‚åœç­‰å¾…ç”¨æˆ·ç¡®è®¤...")
                st.session_state.is_running = False # åœæ­¢è¿è¡Œï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å†æ¬¡ç‚¹å‡»
                st.info(f"ç¬¬ {idx} è½®å®Œæˆã€‚è¯·æŸ¥çœ‹ç»“æœï¼Œå¦‚æœéœ€è¦ä¼˜åŒ–ä¸‹ä¸€ç‰ˆï¼Œè¯·å†æ¬¡ç‚¹å‡»å¼€å§‹ã€‚")
                break
            
            if not rollback_triggered:
                log("ç”Ÿæˆä¼˜åŒ–å»ºè®®ä¸­...")
                optimized_prompt = engine.optimize(current_p, eval_res, {"model": opt_model})
                current_p = optimized_prompt
                st.session_state.current_prompt = optimized_prompt # æ›´æ–°å…¨å±€çŠ¶æ€
        else:
            st.session_state.is_running = False
            st.success("æ‰€æœ‰è¿­ä»£å·²å®Œæˆï¼")

    status_container.update(label="æµç¨‹ç»“æŸ", state="complete", expanded=False)

# ==========================================
# 6. ç»“æœå±•ç¤ºé¢æ¿
# ==========================================

st.divider()

if not st.session_state.history:
    st.info("æš‚æ— æ•°æ®ï¼Œè¯·ç‚¹å‡»å¼€å§‹è°ƒè¯•ã€‚")
else:
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ç»“æœè¯¦æƒ…", "ğŸ“ˆ è¶‹åŠ¿åˆ†æ", "ğŸ“ å·®å¼‚å¯¹æ¯”", "ğŸ“Ÿ è¿è¡Œæ—¥å¿—"])
    
    with tab1:
        # ç‰ˆæœ¬é€‰æ‹©å™¨
        versions = []
        for h in st.session_state.history:
            if 'average_score' in h:
                test_case_count = len(h.get('test_results', []))
                score_text = f"Avg Score: {round(h['average_score'], 1)} ({test_case_count} test cases)"
            else:
                # å…¼å®¹æ—§æ•°æ®æ ¼å¼
                score_text = f"Score: {h['evaluation']['score']}"
            version_text = f"v{h['version']} ({score_text}) {'â†©ï¸' if h['status']=='rolled_back' else ''}"
            versions.append(version_text)

        sel_idx = st.selectbox("é€‰æ‹©ç‰ˆæœ¬æŸ¥çœ‹", range(len(versions)), format_func=lambda x: versions[x])
        data = st.session_state.history[sel_idx]
        
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("#### ğŸ“ æç¤ºè¯æ¨¡æ¿")
            st.code(data['prompt_template'], language='markdown')

            # æ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„ç»“æœ
            st.markdown("#### ğŸ¤– æµ‹è¯•ç»“æœ")
            if 'test_results' in data:
                for test_result in data['test_results']:
                    with st.expander(f"æµ‹è¯•ç”¨ä¾‹: {test_result['test_case']} (å¾—åˆ†: {test_result['score']})"):
                        st.markdown("**å˜é‡:**")
                        st.code(test_result.get('variables', '{}'), language='json')
                        st.markdown("**å®é™…è¾“å‡º:**")
                        st.info(test_result['output'])
                        st.markdown("**é¢„æœŸè¾“å‡º:**")
                        st.text(test_result.get('expected', ''))
                        st.markdown("**è¯„ä»·ç†ç”±:**")
                        st.warning(test_result['result'].get('reason', 'æ— '))
            else:
                # å…¼å®¹æ—§æ•°æ®æ ¼å¼
                st.info(data.get('output', 'æ— è¾“å‡º'))

        with c2:
            st.markdown("#### ğŸ† è´¨é‡è¯„ä¼°")
            if 'average_score' in data:
                st.metric("å¹³å‡å¾—åˆ†", round(data['average_score'], 1), delta=None)
            else:
                # å…¼å®¹æ—§æ•°æ®æ ¼å¼
                score = data['evaluation']['score']
                st.metric("å¾—åˆ†", score, delta=None)

            st.warning(f"**ä¸»è¦è¯„ä»·ç†ç”±**: {data['evaluation']['reason']}")

            st.markdown("---")
            st.markdown("#### ğŸ”§ äººå·¥å¹²é¢„")
            # å…è®¸ç”¨æˆ·åŸºäºæ­¤ç‰ˆæœ¬ä¿®æ”¹
            manual_edit = st.text_area("åœ¨æ­¤ç‰ˆæœ¬åŸºç¡€ä¸Šä¿®æ”¹:", value=data['prompt_template'], key=f"manual_{sel_idx}")
            if st.button("é‡‡çº³æ­¤ä¿®æ”¹å¹¶è¦†ç›–è¾“å…¥æ¡†", key=f"btn_{sel_idx}"):
                st.session_state.current_prompt = manual_edit
                st.rerun()

    with tab2:
        if len(st.session_state.history) > 0:
            # ä½¿ç”¨å¹³å‡åˆ†ç»˜åˆ¶è¶‹åŠ¿å›¾
            scores = []
            for h in st.session_state.history:
                if 'average_score' in h:
                    scores.append(h['average_score'])
                else:
                    # å…¼å®¹æ—§æ•°æ®æ ¼å¼
                    scores.append(h['evaluation']['score'])

            st.line_chart(scores)

            # æ•°æ®è¡¨å¯¼å‡º
            import pandas as pd
            df_data = []
            for h in st.session_state.history:
                if 'average_score' in h:
                    df_data.append({
                        "Version": h['version'],
                        "Avg Score": round(h['average_score'], 1),
                        "Test Cases": len(h.get('test_results', [])),
                        "Status": h['status']
                    })
                else:
                    # å…¼å®¹æ—§æ•°æ®æ ¼å¼
                    df_data.append({
                        "Version": h['version'],
                        "Score": h['evaluation']['score'],
                        "Output": h.get('output', '')[:50]+"...",
                        "Status": h['status']
                    })

            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)

    with tab3:
        if len(st.session_state.history) >= 2:
            v_a = st.selectbox("ç‰ˆæœ¬ A", range(len(versions)), index=len(versions)-2, key="diff_a")
            v_b = st.selectbox("ç‰ˆæœ¬ B", range(len(versions)), index=len(versions)-1, key="diff_b")
            
            txt_a = st.session_state.history[v_a]['prompt_template']
            txt_b = st.session_state.history[v_b]['prompt_template']
            
            st.markdown("### å·®å¼‚è§†å›¾ (A vs B)")
            st.markdown(get_diff_html(txt_a, txt_b), unsafe_allow_html=True)
        else:
            st.warning("éœ€è¦è‡³å°‘ä¸¤ä¸ªç‰ˆæœ¬æ‰èƒ½è¿›è¡Œå¯¹æ¯”")

    with tab4:
        st.text_area("ç³»ç»Ÿæ—¥å¿—", value="\n".join(st.session_state.logs), height=300)

    # å¯¼å‡ºæŒ‰é’®
    report_data = json.dumps(st.session_state.history, indent=2, ensure_ascii=False)
    st.download_button("ğŸ’¾ å¯¼å‡ºå®Œæ•´æŠ¥å‘Š (JSON)", report_data, file_name="debug_report.json", mime="application/json")