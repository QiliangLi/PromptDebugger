import streamlit as st
import json
import os
import time
import datetime
import uuid
import copy
from jinja2 import Template
import difflib
import random

# --- é…ç½®ä¸å¸¸é‡ ---
SESSION_DIR = "prompt_sessions"
PRESET_DIR = "prompt_presets"
os.makedirs(SESSION_DIR, exist_ok=True)
os.makedirs(PRESET_DIR, exist_ok=True)

st.set_page_config(page_title="PromptCraft - æç¤ºè¯è°ƒè¯•å·¥ä½œå°", layout="wide", page_icon="ğŸ”§")

# --- æ ¸å¿ƒé€»è¾‘ç±» ---

class DataManager:
    """æ•°æ®æŒä¹…åŒ–å±‚ï¼šå¤„ç†æ–‡ä»¶å­˜å‚¨"""
    @staticmethod
    def save_session(session_data, session_name=None):
        if not session_name:
            session_name = f"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        path = os.path.join(SESSION_DIR, f"{session_name}.json")
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, indent=2, ensure_ascii=False)
        return path

    @staticmethod
    def list_sessions():
        files = [f for f in os.listdir(SESSION_DIR) if f.endswith('.json')]
        return sorted(files, reverse=True)

class LLMEngine:
    """LLMè°ƒç”¨ä»£ç†æœåŠ¡"""
    def __init__(self, api_key, base_url="https://api.openai.com/v1"):
        self.api_key = api_key
        # è¿™é‡Œé¢„ç•™å®é™…çš„OpenAIè°ƒç”¨æ¥å£
        # å®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨ openai.OpenAI client
        
    def execute(self, prompt, model_config, mock=False):
        """æ‰§è¡ŒLLMç”Ÿæˆå›ç­”"""
        if mock:
            time.sleep(1) # æ¨¡æ‹Ÿå»¶è¿Ÿ
            return f"è¿™æ˜¯åŸºäºæç¤ºè¯äº§ç”Ÿçš„æ¨¡æ‹Ÿå›ç­”ã€‚\n\næç¤ºè¯é‡ç‚¹ï¼š{prompt[:20]}...\nè®¾å®šå‚æ•°ï¼š{model_config['temperature']}"
        # å®ç°çœŸå®çš„APIè°ƒç”¨...
        return "è¯·åœ¨è®¾ç½®ä¸­å…³é—­æ¨¡æ‹Ÿæ¨¡å¼ä»¥è¿æ¥çœŸå®API"

    def evaluate(self, question, actual_output, expected_output, model_config, mock=False):
        """è¯„ä»·LLMï¼šæ‰“åˆ†å¹¶ç»™å‡ºç†ç”±"""
        if mock:
            time.sleep(1)
            score = random.randint(60, 95)
            return {
                "score": score,
                "reason": f"æ¨¡æ‹Ÿè¯„åˆ†ï¼š{score}ã€‚å›ç­”ç»“æ„æ¸…æ™°ï¼Œä½†ç»†èŠ‚ç•¥æœ‰å·®å¼‚ã€‚ä¸é¢„æœŸç»“æœçš„åŒ¹é…åº¦å°šå¯ã€‚"
            }
        # å®ç°çœŸå®çš„è¯„ä»·Prompt...
        return {"score": 0, "reason": "æœªè¿æ¥API"}

    def optimize(self, current_prompt, evaluation_result, model_config, mock=False):
        """ä¼˜åŒ–LLMï¼šç”Ÿæˆæ–°çš„æç¤ºè¯å»ºè®®"""
        if mock:
            time.sleep(1)
            return f"{current_prompt}\n\n[ä¼˜åŒ–å»ºè®®]ï¼šå¢åŠ å…·ä½“çš„è¾“å‡ºæ ¼å¼é™åˆ¶ï¼Œå¹¶å¼ºè°ƒè¯­æ°”æ›´åŠ ä¸“ä¸šã€‚"
        # å®ç°çœŸå®çš„ä¼˜åŒ–Prompt...
        return current_prompt

# --- è¾…åŠ©å‡½æ•° ---

def render_prompt(template_str, variables_json):
    try:
        if not variables_json.strip():
            return template_str
        vars_dict = json.loads(variables_json)
        template = Template(template_str)
        return template.render(**vars_dict)
    except Exception as e:
        return f"Error rendering template: {str(e)}"

def get_diff_html(text1, text2):
    """ç”ŸæˆHTMLæ ¼å¼çš„æ–‡æœ¬å·®å¼‚å¯¹æ¯”"""
    d = difflib.Differ()
    diff = list(d.compare(text1.splitlines(), text2.splitlines()))
    html = []
    for line in diff:
        if line.startswith('+ '):
            html.append(f'<div style="background-color: #e6ffec; color: #155724;">{line}</div>')
        elif line.startswith('- '):
            html.append(f'<div style="background-color: #ffe6e6; color: #721c24;">{line}</div>')
        elif line.startswith('? '):
            continue
        else:
            html.append(f'<div style="color: #666;">{line}</div>')
    return "".join(html)

# --- ç•Œé¢çŠ¶æ€ç®¡ç† ---

if 'history' not in st.session_state:
    st.session_state.history = [] # å­˜å‚¨æ¯æ¬¡è¿­ä»£çš„è¯¦ç»†è®°å½•
if 'current_version' not in st.session_state:
    st.session_state.current_version = 0
if 'is_running' not in st.session_state:
    st.session_state.is_running = False
if 'logs' not in st.session_state:
    st.session_state.logs = []

def log(message):
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    st.session_state.logs.append(f"[{timestamp}] {message}")

# --- ä¾§è¾¹æ ï¼šé…ç½®ç®¡ç† ---

with st.sidebar:
    st.header("âš™ï¸ å…¨å±€é…ç½®")
    
    # æ¨¡æ‹Ÿæ¨¡å¼å¼€å…³
    use_mock = st.toggle("å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼ (æ— éœ€API Key)", value=True)
    
    st.divider()
    
    st.subheader("ğŸ¤– æ¨¡å‹é…ç½®")
    
    with st.expander("1. æ‰§è¡Œæ¨¡å‹ (Executor)", expanded=True):
        exec_model = st.selectbox("Model", ["gpt-4o", "gpt-3.5-turbo", "claude-3-5-sonnet"], key="exec_m")
        exec_temp = st.slider("Temperature", 0.0, 1.0, 0.7, key="exec_t")
    
    with st.expander("2. è¯„ä»·æ¨¡å‹ (Evaluator)"):
        eval_model = st.selectbox("Model", ["gpt-4o", "gpt-4-turbo"], key="eval_m")
        st.caption("è´Ÿè´£ä¸ºæ‰§è¡Œç»“æœæ‰“åˆ† (0-100)")

    with st.expander("3. ä¼˜åŒ–æ¨¡å‹ (Optimizer)"):
        opt_model = st.selectbox("Model", ["gpt-4o", "gpt-4-turbo"], key="opt_m")
        st.caption("è´Ÿè´£æ ¹æ®è¯„ä»·ç»“æœä¿®æ”¹æç¤ºè¯")

    st.divider()
    st.subheader("ğŸ’¾ æ¨¡æ¿ç®¡ç†")
    preset_name = st.text_input("ä¿å­˜å½“å‰é…ç½®ä¸ºæ¨¡æ¿")
    if st.button("ä¿å­˜æ¨¡æ¿"):
        st.success(f"æ¨¡æ¿ {preset_name} å·²ä¿å­˜ (æ¨¡æ‹Ÿ)")

# --- ä¸»ç•Œé¢ ---

st.title("ğŸš€ æç¤ºè¯æ™ºèƒ½è°ƒè¯•å·¥ä½œå°")

# 1. æ ¸å¿ƒè¾“å…¥åŒºåŸŸ
col1, col2 = st.columns([3, 2])

with col1:
    st.subheader("ğŸ“ æç¤ºè¯è®¾è®¡ (Prompt Template)")
    prompt_input = st.text_area(
        "æ”¯æŒ {{variable}} è¯­æ³•", 
        height=300, 
        value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚\nè¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼š\n\n{{source_text}}",
        key="prompt_input_area"
    )

with col2:
    st.subheader("ğŸ§© å˜é‡ä¸é¢„æœŸ")
    variables_input = st.text_area("å˜é‡ (JSONæ ¼å¼)", value='{\n  "source_text": "To be or not to be, that is the question."\n}', height=120)
    expected_output = st.text_area("é¢„æœŸå›ç­” (ç”¨äºè‡ªåŠ¨è¯„åˆ†)", value="ç”Ÿå­˜è¿˜æ˜¯æ¯ç­ï¼Œè¿™æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚", height=120)
    
    st.info("ğŸ’¡ æç¤ºï¼šåœ¨å·¦ä¾§ä½¿ç”¨ {{variable}} æ ‡è®°å¯å˜éƒ¨åˆ†ï¼Œåœ¨å³ä¾§JSONä¸­å®šä¹‰å…·ä½“å€¼ã€‚")

# 2. æ§åˆ¶å°ä¸è¿­ä»£è®¾ç½®
st.divider()

ctrl_col1, ctrl_col2, ctrl_col3, ctrl_col4 = st.columns(4)

with ctrl_col1:
    iteration_mode = st.radio("è¿­ä»£æ¨¡å¼", ["è‡ªåŠ¨è¿­ä»£", "äº¤äº’å¼è¿­ä»£"], horizontal=True)

with ctrl_col2:
    max_iterations = st.number_input("æœ€å¤§è¿­ä»£æ¬¡æ•°", min_value=1, max_value=10, value=3)

with ctrl_col3:
    auto_rollback = st.checkbox("å¯ç”¨æ™ºèƒ½å›æ»š", value=True, help="å½“æ–°ç‰ˆæœ¬å¾—åˆ†ä½äºæ—§ç‰ˆæœ¬æ—¶ï¼Œè‡ªåŠ¨æ¢å¤")

with ctrl_col4:
    start_btn = st.button("â–¶ï¸ å¼€å§‹è°ƒè¯•", type="primary", use_container_width=True)
    reset_btn = st.button("ğŸ”„ é‡ç½®ä¼šè¯", use_container_width=True)

if reset_btn:
    st.session_state.history = []
    st.session_state.logs = []
    st.session_state.current_version = 0
    st.rerun()

# --- æ ¸å¿ƒå¤„ç†é€»è¾‘ ---

llm_engine = LLMEngine(api_key="mock" if use_mock else os.getenv("OPENAI_API_KEY"))

if start_btn:
    st.session_state.is_running = True
    current_prompt = prompt_input
    
    # åˆå§‹åŒ–/ç»§ç»­è¿­ä»£å¾ªç¯
    progress_bar = st.progress(0)
    
    for i in range(max_iterations):
        iter_num = i + 1
        log(f"--- å¼€å§‹è¿­ä»£ #{iter_num} ---")
        
        # 1. æ¸²æŸ“å˜é‡
        final_prompt = render_prompt(current_prompt, variables_input)
        
        # 2. æ‰§è¡Œè°ƒç”¨
        log("æ­£åœ¨è°ƒç”¨æ‰§è¡Œæ¨¡å‹...")
        actual_output = llm_engine.execute(
            final_prompt, 
            {"model": exec_model, "temperature": exec_temp}, 
            mock=use_mock
        )
        
        # 3. è´¨é‡è¯„ä¼°
        log("æ­£åœ¨è¯„ä¼°ç»“æœè´¨é‡...")
        eval_result = llm_engine.evaluate(
            final_prompt, actual_output, expected_output,
            {"model": eval_model}, mock=use_mock
        )
        score = eval_result['score']
        log(f"å½“å‰å¾—åˆ†: {score}/100")
        
        # è®°å½•æœ¬æ¬¡è¿­ä»£æ•°æ®
        iteration_data = {
            "version": iter_num,
            "prompt_template": current_prompt,
            "final_prompt": final_prompt,
            "output": actual_output,
            "evaluation": eval_result,
            "timestamp": datetime.datetime.now().isoformat()
        }
        
        # 4. æ™ºèƒ½å›æ»šé€»è¾‘
        rollback_triggered = False
        if auto_rollback and len(st.session_state.history) > 0:
            last_score = st.session_state.history[-1]['evaluation']['score']
            if score < last_score:
                log(f"âš ï¸ è­¦å‘Š: è´¨é‡ä¸‹é™ ({score} < {last_score})ï¼Œè§¦å‘å›æ»šã€‚")
                rollback_triggered = True
                # å›æ»šæç¤ºè¯åˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬ï¼Œä½†è®°å½•è¿™æ¬¡å¤±è´¥çš„å°è¯•
                iteration_data['status'] = 'rolled_back'
                current_prompt = st.session_state.history[-1]['prompt_template']
            else:
                iteration_data['status'] = 'accepted'
        else:
            iteration_data['status'] = 'accepted'
            
        st.session_state.history.append(iteration_data)
        progress_bar.progress((i + 1) / max_iterations)
        
        # 5. ä¼˜åŒ–é€»è¾‘ (å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡)
        if i < max_iterations - 1:
            if iteration_mode == "äº¤äº’å¼è¿­ä»£":
                st.session_state.awaiting_user = True
                # åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œè¿™é‡Œéœ€è¦ä¸€ç§æœºåˆ¶æ‰“æ–­å¾ªç¯ç­‰å¾…ç”¨æˆ·è¾“å…¥
                # Streamlitçš„æœºåˆ¶å¾ˆéš¾åœ¨å¾ªç¯ä¸­ç›´æ¥æš‚åœï¼Œé€šå¸¸é€šè¿‡rerunå¤„ç†
                # æ­¤å¤„ç®€åŒ–å¤„ç†ï¼šäº¤äº’æ¨¡å¼ä¸‹æ¯æ¬¡åªè·‘ä¸€è½®ï¼Œéœ€è¦ç”¨æˆ·å†æ¬¡ç‚¹å‡»ç»§ç»­
                log("äº¤äº’æ¨¡å¼ï¼šç­‰å¾…ç”¨æˆ·ç¡®è®¤ä¸‹ä¸€æ­¥...")
                break 
            
            if not rollback_triggered:
                log("æ­£åœ¨ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
                current_prompt = llm_engine.optimize(
                    current_prompt, eval_result, 
                    {"model": opt_model}, mock=use_mock
                )
        
    st.session_state.is_running = False
    st.success("è°ƒè¯•æµç¨‹å®Œæˆï¼")

# --- ç»“æœå¯è§†åŒ–åŒºåŸŸ ---

st.divider()
st.subheader("ğŸ“Š è°ƒè¯•æŠ¥å‘Š")

if not st.session_state.history:
    st.info("æš‚æ— è°ƒè¯•æ•°æ®ï¼Œè¯·ç‚¹å‡»å¼€å§‹è°ƒè¯•ã€‚")
else:
    # å†å²è®°å½•é€‰é¡¹å¡
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” ç»“æœè¯¦æƒ…", "ğŸ“ˆ è¿­ä»£å†å²è¶‹åŠ¿", "â†”ï¸ æç¤ºè¯å·®å¼‚å¯¹æ¯”", "ğŸ“Ÿ è¿è¡Œæ—¥å¿—"])
    
    with tab1:
        # é€‰æ‹©è¦æŸ¥çœ‹çš„ç‰ˆæœ¬
        versions = [f"v{h['version']} (Score: {h['evaluation']['score']})" for h in st.session_state.history]
        selected_v_idx = st.selectbox("é€‰æ‹©ç‰ˆæœ¬æŸ¥çœ‹è¯¦æƒ…", range(len(versions)), format_func=lambda x: versions[x])
        
        sel_data = st.session_state.history[selected_v_idx]
        
        r_col1, r_col2 = st.columns(2)
        with r_col1:
            st.markdown("#### æç¤ºè¯ (Prompt)")
            st.code(sel_data['prompt_template'], language="markdown")
            st.markdown(f"**çŠ¶æ€:** `{sel_data['status']}`")
            
        with r_col2:
            st.markdown("#### è¾“å‡ºç»“æœ (Output)")
            st.info(sel_data['output'])
            st.markdown("#### è¯„ä»· (Evaluation)")
            st.metric("è´¨é‡è¯„åˆ†", sel_data['evaluation']['score'])
            st.warning(f"è¯„ä»·ç†ç”±: {sel_data['evaluation']['reason']}")

            # äººå·¥å¹²é¢„åŒº
            st.markdown("---")
            st.markdown("**äººå·¥ä¼˜åŒ–è·¯å¾„**")
            new_manual_prompt = st.text_area("åŸºäºæ­¤ç‰ˆæœ¬æ‰‹åŠ¨ä¿®æ”¹", value=sel_data['prompt_template'], key="manual_edit")
            if st.button("é‡‡çº³æ­¤æ‰‹åŠ¨ä¿®æ”¹å¹¶æ›´æ–°è¾“å…¥æ¡†"):
                st.session_state.prompt_input_area = new_manual_prompt # æ³¨æ„ï¼šè¿™éœ€è¦å›è°ƒæŠ€å·§æˆ–rerunæ‰èƒ½ç”Ÿæ•ˆ
                st.info("æç¤ºè¯å·²æ›´æ–°åˆ°ä¸Šæ–¹è¾“å…¥æ¡†")

    with tab2:
        # ä½¿ç”¨ç®€å•çš„å›¾è¡¨æ˜¾ç¤ºåˆ†æ•°è¶‹åŠ¿
        scores = [h['evaluation']['score'] for h in st.session_state.history]
        st.line_chart(scores)
        st.dataframe(st.session_state.history)

    with tab3:
        if len(st.session_state.history) > 1:
            v_a_idx = st.selectbox("ç‰ˆæœ¬ A", range(len(versions)), index=0, key="diff_a")
            v_b_idx = st.selectbox("ç‰ˆæœ¬ B", range(len(versions)), index=len(versions)-1, key="diff_b")
            
            prompt_a = st.session_state.history[v_a_idx]['prompt_template']
            prompt_b = st.session_state.history[v_b_idx]['prompt_template']
            
            st.markdown("### å·®å¼‚è§†å›¾ (Version A vs Version B)")
            diff_html = get_diff_html(prompt_a, prompt_b)
            st.markdown(diff_html, unsafe_allow_html=True)
        else:
            st.write("éœ€è¦è‡³å°‘ä¸¤æ¬¡è¿­ä»£æ‰èƒ½è¿›è¡Œå¯¹æ¯”ã€‚")

    with tab4:
        for log_msg in st.session_state.logs:
            st.text(log_msg)

    # å¯¼å‡ºåŠŸèƒ½
    st.divider()
    report_json = json.dumps(st.session_state.history, indent=2, ensure_ascii=False)
    st.download_button(
        label="ğŸ“¥ å¯¼å‡ºå®Œæ•´è°ƒè¯•æŠ¥å‘Š (JSON)",
        data=report_json,
        file_name="prompt_debug_report.json",
        mime="application/json"
    )