import streamlit as st
import time
import json
import random
import difflib
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any, Optional

# ==========================================
# 1. é…ç½®ä¸åˆå§‹åŒ– (Configuration & Init)
# ==========================================

st.set_page_config(
    page_title="PromptCraft - æç¤ºè¯æ™ºèƒ½è°ƒè¯•å°",
    page_icon="ğŸ› ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS æ ·å¼
st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Courier New', monospace; }
    .diff-added { background-color: #e6ffec; color: #24292e; padding: 2px; }
    .diff-removed { background-color: #ffebe9; color: #24292e; text-decoration: line-through; padding: 2px; }
    .metric-card { background-color: #f0f2f6; padding: 10px; border-radius: 5px; border-left: 4px solid #4e8cff; }
    .status-running { color: orange; font-weight: bold; }
    .status-idle { color: green; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

# çŠ¶æ€ç®¡ç†åˆå§‹åŒ–
if 'history' not in st.session_state:
    st.session_state.history = []
if 'current_prompt' not in st.session_state:
    st.session_state.current_prompt = ""
if 'current_version' not in st.session_state:
    st.session_state.current_version = 0
if 'is_running' not in st.session_state:
    st.session_state.is_running = False
if 'optimization_mode' not in st.session_state:
    st.session_state.optimization_mode = "LLMè‡ªåŠ¨ä¼˜åŒ–" # æˆ– "äººå·¥ä¼˜åŒ–"

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ç±» (Core Logic Classes)
# ==========================================

class MockLLMService:
    """
    æ¨¡æ‹Ÿ LLM æœåŠ¡ (æ”¯æŒæ‰§è¡Œã€è¯„ä»·ã€ä¼˜åŒ–)
    åœ¨å®é™…ç”Ÿäº§ä¸­ï¼Œè¿™é‡Œæ›¿æ¢ä¸º OpenAI/Anthropic/Gemini çš„ API è°ƒç”¨
    """
    @staticmethod
    def execute_prompt(prompt: str, variables: Dict[str, str], model_config: Dict) -> str:
        # æ¨¡æ‹Ÿ API å»¶è¿Ÿ
        time.sleep(0.5) 
        filled_prompt = prompt
        for k, v in variables.items():
            filled_prompt = filled_prompt.replace(f"{{{{{k}}}}}", v)
        
        return f"[æ¨¡æ‹ŸLLMå›ç­”] åŸºäºæç¤ºè¯é•¿åº¦ {len(filled_prompt)} çš„ç”Ÿæˆç»“æœã€‚\næ ¸å¿ƒå†…å®¹ï¼š{filled_prompt[:20]}..."

    @staticmethod
    def evaluate_quality(output: str, expected: str, model_config: Dict) -> float:
        # æ¨¡æ‹Ÿè¯„åˆ† (0-10)
        base_score = random.uniform(6.0, 9.5)
        # ç®€å•çš„æ¨¡æ‹Ÿé€»è¾‘ï¼šå¦‚æœè¾“å‡ºåŒ…å«é¢„æœŸå…³é”®è¯ï¼Œåˆ†æ•°æ›´é«˜
        if expected and expected in output:
            base_score += 0.5
        return min(10.0, round(base_score, 1))

    @staticmethod
    def optimize_prompt(current_prompt: str, feedback: str, model_config: Dict) -> str:
        # æ¨¡æ‹Ÿä¼˜åŒ–ï¼šåœ¨æœ«å°¾æ·»åŠ ä¸€äº›ä¿®é¥°è¯
        modifiers = ["è¯·æ›´ç®€æ´ä¸€ç‚¹ã€‚", "ä½¿ç”¨æ›´ä¸“ä¸šçš„æœ¯è¯­ã€‚", "è¯·åˆ†ç‚¹è®ºè¿°ã€‚", "æ³¨æ„è¯­æ°”è¦æ¸©å’Œã€‚"]
        chosen = random.choice(modifiers)
        return f"{current_prompt}\n\n[ä¼˜åŒ–æŒ‡ä»¤]: {chosen}"

class SessionManager:
    """ç®¡ç†è°ƒè¯•ä¼šè¯å’Œå†å²è®°å½•"""
    @staticmethod
    def save_history(history: List[Dict]):
        filename = f"debug_session_{datetime.now().strftime('%Y%m%d')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        return filename

    @staticmethod
    def generate_diff(text1: str, text2: str) -> str:
        d = difflib.Differ()
        diff = list(d.compare(text1.splitlines(), text2.splitlines()))
        html_diff = ""
        for line in diff:
            if line.startswith('+ '):
                html_diff += f"<div class='diff-added'>{line[2:]}</div>"
            elif line.startswith('- '):
                html_diff += f"<div class='diff-removed'>{line[2:]}</div>"
            elif line.startswith('  '):
                html_diff += f"<div>{line[2:]}</div>"
        return html_diff

# ==========================================
# 3. ä¾§è¾¹æ é…ç½® (Sidebar Configuration)
# ==========================================

with st.sidebar:
    st.header("âš™ï¸ è°ƒè¯•æ§åˆ¶å°")
    
    st.subheader("1. è¿­ä»£è®¾ç½®")
    iter_mode = st.radio("è¿­ä»£æ¨¡å¼", ["è‡ªåŠ¨è¿­ä»£ (è¿ç»­)", "äº¤äº’å¼ (äººå·¥ç¡®è®¤)"])
    max_iters = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 1, 10, 3)
    
    st.subheader("2. LLM é…ç½®")
    with st.expander("API å‚æ•°è®¾ç½®"):
        exec_model = st.selectbox("æ‰§è¡Œæ¨¡å‹", ["gpt-4o", "gemini-1.5-pro", "claude-3-5-sonnet"])
        eval_model = st.selectbox("è¯„ä»·æ¨¡å‹", ["gpt-4o", "gpt-3.5-turbo"])
        temp = st.slider("Temperature", 0.0, 1.0, 0.7)
    
    st.subheader("3. ç­–ç•¥é…ç½®")
    rollback_enabled = st.checkbox("å¯ç”¨è´¨é‡å›æ»š (åˆ†æ•°ä¸‹é™æ—¶)", value=True)
    rollback_threshold = st.number_input("å›æ»šé˜ˆå€¼ (åˆ†å·®)", 0.1, 2.0, 0.5)

    st.divider()
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²è®°å½•"):
        st.session_state.history = []
        st.session_state.current_version = 0
        st.rerun()

# ==========================================
# 4. ä¸»ç•Œé¢ (Main Interface)
# ==========================================

st.title("ğŸ§© Prompt Debugger Pro")

# -----------------
# Tab 1: è¾“å…¥ä¸è®¾è®¡
# -----------------
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ğŸ“ æç¤ºè¯è®¾è®¡ (Prompt)")
    st.caption("ä½¿ç”¨ {{variable}} æ ‡è®°å¯å˜éƒ¨åˆ†")
    
    # é»˜è®¤æ¨¡æ¿
    default_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚
è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{{target_language}}ï¼š
å†…å®¹ï¼š{{source_text}}

è¦æ±‚ï¼š
1. ä¿æŒåŸæ„
2. é£æ ¼ä¼˜é›…"""
    
    prompt_input = st.text_area(
        "Prompt Template", 
        value=st.session_state.get('current_prompt') or default_prompt, 
        height=300,
        key="prompt_editor"
    )
    st.session_state.current_prompt = prompt_input

with col2:
    st.subheader("ğŸ§ª æµ‹è¯•ç”¨ä¾‹ (Variables)")
    
    # è§£æå˜é‡
    import re
    vars_found = re.findall(r"{{(.*?)}}", prompt_input)
    unique_vars = sorted(list(set(vars_found)))
    
    var_inputs = {}
    if unique_vars:
        for v in unique_vars:
            var_inputs[v] = st.text_input(f"å˜é‡: {v}", key=f"var_{v}")
    else:
        st.info("æœªæ£€æµ‹åˆ°å˜é‡ï¼Œå°†ä½œä¸ºé™æ€æç¤ºè¯è¿è¡Œã€‚")
        
    st.subheader("ğŸ¯ é¢„æœŸæ ‡å‡† (Ground Truth)")
    expected_output = st.text_area("é¢„æœŸå›ç­” (ç”¨äºè‡ªåŠ¨è¯„åˆ†å‚è€ƒ)", height=100)

# -----------------
# Tab 2: è°ƒè¯•ä¸æ‰§è¡Œ
# -----------------

st.divider()
st.subheader("ğŸš€ è°ƒè¯•æ‰§è¡Œ")

control_col, status_col = st.columns([1, 3])

with control_col:
    start_btn = st.button("å¼€å§‹è°ƒè¯•æµç¨‹", type="primary", use_container_width=True)
    opt_path = st.selectbox("ä¼˜åŒ–è·¯å¾„", ["LLMè‡ªåŠ¨ä¼˜åŒ–", "äººå·¥ä»‹å…¥ä¿®æ”¹"])

with status_col:
    status_placeholder = st.empty()
    progress_bar = st.progress(0)

# ç»“æœå±•ç¤ºå®¹å™¨
result_container = st.container()

# -----------------
# é€»è¾‘æ‰§è¡Œ (Execution Logic)
# -----------------

if start_btn:
    st.session_state.is_running = True
    current_prompt_ver = prompt_input
    
    for i in range(max_iters):
        # 1. çŠ¶æ€æ›´æ–°
        status_placeholder.markdown(f"**â³ æ­£åœ¨æ‰§è¡Œç¬¬ {i+1}/{max_iters} è½®è¿­ä»£...**")
        progress_bar.progress((i + 1) / max_iters)
        
        # 2. è°ƒç”¨æ‰§è¡Œ LLM
        output = MockLLMService.execute_prompt(
            current_prompt_ver, 
            var_inputs, 
            {"model": exec_model, "temp": temp}
        )
        
        # 3. è°ƒç”¨è¯„ä»· LLM
        score = MockLLMService.evaluate_quality(output, expected_output, {"model": eval_model})
        
        # 4. è®°å½•æ•°æ®
        record = {
            "version": st.session_state.current_version + 1,
            "timestamp": datetime.now().isoformat(),
            "prompt": current_prompt_ver,
            "output": output,
            "score": score,
            "diff_html": "", # ä¸ä¸Šä¸€ç‰ˆæœ¬å¯¹æ¯”
            "status": "Success"
        }
        
        # 5. ç”Ÿæˆ Diff ä¸ å›æ»šé€»è¾‘
        is_rollback = False
        if st.session_state.history:
            prev_record = st.session_state.history[-1]
            record["diff_html"] = SessionManager.generate_diff(prev_record["prompt"], current_prompt_ver)
            
            # å›æ»šæ£€æŸ¥
            if rollback_enabled and (prev_record["score"] - score > rollback_threshold):
                record["status"] = "Rollback Triggered"
                is_rollback = True
                status_placeholder.warning(f"âš ï¸ æ£€æµ‹åˆ°è´¨é‡ä¸‹é™ (Score {score} < {prev_record['score']})ï¼Œè§¦å‘å›æ»šã€‚")
                current_prompt_ver = prev_record["prompt"] # æ¢å¤æ—§æç¤ºè¯
        
        st.session_state.history.append(record)
        st.session_state.current_version += 1
        
        # 6. å±•ç¤ºå½“å‰ç»“æœ
        with result_container:
            with st.expander(f"Iter #{i+1} - Score: {score} - {record['status']}", expanded=True):
                c1, c2 = st.columns([1, 1])
                with c1:
                    st.markdown("**ç”Ÿæˆç»“æœ:**")
                    st.info(output)
                with c2:
                    st.markdown("**Prompt å˜æ›´:**")
                    if record["diff_html"]:
                        st.markdown(record["diff_html"], unsafe_allow_html=True)
                    else:
                        st.caption("åˆå§‹ç‰ˆæœ¬")
        
        # 7. ä¼˜åŒ–é˜¶æ®µ (å‡†å¤‡ä¸‹ä¸€è½®)
        if i < max_iters - 1 and not is_rollback:
            if iter_mode == "äº¤äº’å¼ (äººå·¥ç¡®è®¤)":
                st.session_state.is_running = False
                status_placeholder.success(f"ç¬¬ {i+1} è½®å®Œæˆã€‚ç­‰å¾…äººå·¥ç¡®è®¤...")
                break # å®é™…åº”ç”¨ä¸­è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„æš‚åœé€»è¾‘ï¼ŒStreamlit ä¸­é€šå¸¸é€šè¿‡ rerun å®ç°
            
            # è‡ªåŠ¨ä¼˜åŒ–é€»è¾‘
            if opt_path == "LLMè‡ªåŠ¨ä¼˜åŒ–":
                current_prompt_ver = MockLLMService.optimize_prompt(current_prompt_ver, output, {})
                status_placeholder.info("ğŸ¤– AI æ­£åœ¨ä¼˜åŒ–æç¤ºè¯...")
                time.sleep(1)
            else:
                # å¦‚æœæ˜¯äººå·¥è·¯å¾„ä¸”è‡ªåŠ¨æ¨¡å¼ï¼Œè¿™é‡Œä¸ºäº†æ¼”ç¤ºç»§ç»­å¾ªç¯ï¼Œå®é™…åº”æš‚åœ
                pass
                
    st.session_state.is_running = False
    status_placeholder.success("âœ… è°ƒè¯•æµç¨‹ç»“æŸ")


# ==========================================
# 5. å†å²ä¸åˆ†æ (History & Analysis)
# ==========================================

if st.session_state.history:
    st.divider()
    st.header("ğŸ“Š è¿­ä»£åˆ†ææŠ¥å‘Š")
    
    # æŒ‡æ ‡è¶‹åŠ¿å›¾
    hist_df = pd.DataFrame(st.session_state.history)
    st.line_chart(hist_df, x="version", y="score")
    
    # å†å²è®°å½•è¡¨æ ¼
    st.dataframe(
        hist_df[["version", "score", "status", "timestamp"]],
        use_container_width=True
    )
    
    # å¯¼å‡ºåŠŸèƒ½
    col_export, col_dummy = st.columns([1, 4])
    with col_export:
        history_json = json.dumps(st.session_state.history, indent=2, ensure_ascii=False)
        st.download_button(
            label="ğŸ“¥ å¯¼å‡ºè°ƒè¯•æŠ¥å‘Š (JSON)",
            data=history_json,
            file_name=f"prompt_debug_report_{int(time.time())}.json",
            mime="application/json"
        )

# ==========================================
# 6. é¡µè„š
# ==========================================
st.markdown("---")
st.caption("Prompt Debugger Tool v1.0 | Local Mode | No Database Required")